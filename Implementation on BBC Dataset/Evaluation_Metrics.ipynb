{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ROUGE-1**: Measures the overlap of single words between the generated summary and the reference summary.\n",
        "\n",
        "**ROUGE-2**: Measures the overlap of two consecutive words (bigrams) between the generated summary and the reference summary.\n",
        "\n",
        "**ROUGE-L**: Measures the longest common subsequence of words, reflecting sentence-level structure and order between the generated summary and the reference summary.\n",
        "\n",
        "Since we are comparing extractive text summarization to abstractive text summarization, ROUGE metrics are preferred over traditional precision and recall as they provide a more nuanced evaluation of content coverage, contextual coherence, and structural preservation. Traditional precision and recall do not capture the linguistic and structural similarities between the generated and reference summaries, which are crucial for summarization tasks."
      ],
      "metadata": {
        "id": "Xu7YrEAKTTmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryLkCOM-hfYi",
        "outputId": "71d2e0b0-e2b1-45e0-a215-b053ddf8a7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCICPrnVi3Wm",
        "outputId": "43d81e19-1634-4341-eabb-9cbea2d7edde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.5)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=e4c9e2303dd6b8bb348d5c6e6e0933c2439db54810d1b7038e0f22845c204d38\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqCe3pQxDqjB",
        "outputId": "ace3bbea-b3e3-461e-97ea-964f57042dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def calculate_rouge_scores(generated_summaries_folder, reference_summaries_folder):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    rouge1_precision_dict = {}\n",
        "    rouge1_recall_dict = {}\n",
        "    rouge1_f1_dict = {}\n",
        "    rouge2_precision_dict = {}\n",
        "    rouge2_recall_dict = {}\n",
        "    rouge2_f1_dict = {}\n",
        "    rougeL_precision_dict = {}\n",
        "    rougeL_recall_dict = {}\n",
        "    rougeL_f1_dict = {}\n",
        "\n",
        "    for subfolder in os.listdir(generated_summaries_folder):\n",
        "        subfolder_generated_summaries_folder = os.path.join(generated_summaries_folder, subfolder)\n",
        "        subfolder_reference_summaries_folder = os.path.join(reference_summaries_folder, subfolder)\n",
        "\n",
        "\n",
        "        rouge1_precision = 0\n",
        "        rouge1_recall = 0\n",
        "        rouge1_f1 = 0\n",
        "        rouge2_precision = 0\n",
        "        rouge2_recall = 0\n",
        "        rouge2_f1 = 0\n",
        "        rougeL_precision = 0\n",
        "        rougeL_recall = 0\n",
        "        rougeL_f1 = 0\n",
        "\n",
        "        for filename in os.listdir(subfolder_generated_summaries_folder):\n",
        "            reference_summary_file = os.path.join(subfolder_reference_summaries_folder, filename)\n",
        "            generated_summary_file = os.path.join(subfolder_generated_summaries_folder, filename)\n",
        "\n",
        "\n",
        "            with open(reference_summary_file, 'r') as f:\n",
        "                reference_summary = f.read()\n",
        "            with open(generated_summary_file, 'r') as f:\n",
        "                generated_summary = f.read()\n",
        "\n",
        "            scores = scorer.score(reference_summary, generated_summary)\n",
        "            rouge1_precision += scores['rouge1'].precision\n",
        "            rouge1_recall += scores['rouge1'].recall\n",
        "            rouge1_f1 += scores['rouge1'].fmeasure\n",
        "            rouge2_precision += scores['rouge2'].precision\n",
        "            rouge2_recall += scores['rouge2'].recall\n",
        "            rouge2_f1 += scores['rouge2'].fmeasure\n",
        "            rougeL_precision += scores['rougeL'].precision\n",
        "            rougeL_recall += scores['rougeL'].recall\n",
        "            rougeL_f1 += scores['rougeL'].fmeasure\n",
        "\n",
        "        num_files = len(os.listdir(subfolder_generated_summaries_folder))\n",
        "        rouge1_precision_dict[subfolder] = rouge1_precision / num_files\n",
        "        rouge1_recall_dict[subfolder] = rouge1_recall / num_files\n",
        "        rouge1_f1_dict[subfolder] = rouge1_f1 / num_files\n",
        "        rouge2_precision_dict[subfolder] = rouge2_precision / num_files\n",
        "        rouge2_recall_dict[subfolder] = rouge2_recall / num_files\n",
        "        rouge2_f1_dict[subfolder] = rouge2_f1 / num_files\n",
        "        rougeL_precision_dict[subfolder] = rougeL_precision / num_files\n",
        "        rougeL_recall_dict[subfolder] = rougeL_recall / num_files\n",
        "        rougeL_f1_dict[subfolder] = rougeL_f1 / num_files\n",
        "\n",
        "    return (\n",
        "        rouge1_precision_dict, rouge1_recall_dict, rouge1_f1_dict,\n",
        "        rouge2_precision_dict, rouge2_recall_dict, rouge2_f1_dict,\n",
        "        rougeL_precision_dict, rougeL_recall_dict, rougeL_f1_dict\n",
        "    )\n",
        "\n",
        "generated_summaries_folder = '/content/drive/MyDrive/RBM_50/Generated_Summaries'\n",
        "reference_summaries_folder = '/content/drive/MyDrive/zip_ref/BBC News Summary/Summaries'\n",
        "\n",
        "(\n",
        "    rouge1_precision_dict, rouge1_recall_dict, rouge1_f1_dict,\n",
        "    rouge2_precision_dict, rouge2_recall_dict, rouge2_f1_dict,\n",
        "    rougeL_precision_dict, rougeL_recall_dict, rougeL_f1_dict\n",
        ") = calculate_rouge_scores(generated_summaries_folder, reference_summaries_folder)\n",
        "\n",
        "\n",
        "for subfolder in rouge1_precision_dict:\n",
        "    print(f\"Subfolder: {subfolder}\")\n",
        "    print(\"ROUGE-1 Precision:\", rouge1_precision_dict[subfolder])\n",
        "    print(\"ROUGE-1 Recall:\", rouge1_recall_dict[subfolder])\n",
        "    print(\"ROUGE-1 F1:\", rouge1_f1_dict[subfolder])\n",
        "    print(\"ROUGE-2 Precision:\", rouge2_precision_dict[subfolder])\n",
        "    print(\"ROUGE-2 Recall:\", rouge2_recall_dict[subfolder])\n",
        "    print(\"ROUGE-2 F1:\", rouge2_f1_dict[subfolder])\n",
        "    print(\"ROUGE-L Precision:\", rougeL_precision_dict[subfolder])\n",
        "    print(\"ROUGE-L Recall:\", rougeL_recall_dict[subfolder])\n",
        "    print(\"ROUGE-L F1:\", rougeL_f1_dict[subfolder])\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "X91UQyShlOBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a47126-5147-4092-f223-1111dcbb770a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subfolder: business\n",
            "ROUGE-1 Precision: 0.6680125626837117\n",
            "ROUGE-1 Recall: 0.5519826486674696\n",
            "ROUGE-1 F1: 0.5852230408561466\n",
            "ROUGE-2 Precision: 0.5279428932663398\n",
            "ROUGE-2 Recall: 0.4444153974619055\n",
            "ROUGE-2 F1: 0.46709493738175906\n",
            "ROUGE-L Precision: 0.4583901912137169\n",
            "ROUGE-L Recall: 0.382576347871064\n",
            "ROUGE-L F1: 0.40357866368453876\n",
            "\n",
            "Subfolder: entertainment\n",
            "ROUGE-1 Precision: 0.6479118313444703\n",
            "ROUGE-1 Recall: 0.5517011572963691\n",
            "ROUGE-1 F1: 0.5734227935889803\n",
            "ROUGE-2 Precision: 0.5108076201551278\n",
            "ROUGE-2 Recall: 0.44004835818292737\n",
            "ROUGE-2 F1: 0.45519858316228573\n",
            "ROUGE-L Precision: 0.4543103772349623\n",
            "ROUGE-L Recall: 0.3897767944597413\n",
            "ROUGE-L F1: 0.4035663792357023\n",
            "\n",
            "Subfolder: politics\n",
            "ROUGE-1 Precision: 0.694180232316015\n",
            "ROUGE-1 Recall: 0.4382797392593151\n",
            "ROUGE-1 F1: 0.5165392868747863\n",
            "ROUGE-2 Precision: 0.517131880068427\n",
            "ROUGE-2 Recall: 0.3316500560864897\n",
            "ROUGE-2 F1: 0.387885666848406\n",
            "ROUGE-L Precision: 0.452213697410132\n",
            "ROUGE-L Recall: 0.2881906875558029\n",
            "ROUGE-L F1: 0.3379333957148768\n",
            "\n",
            "Subfolder: sport\n",
            "ROUGE-1 Precision: 0.6366619649267968\n",
            "ROUGE-1 Recall: 0.521153740956198\n",
            "ROUGE-1 F1: 0.5421661905164616\n",
            "ROUGE-2 Precision: 0.49522510891355187\n",
            "ROUGE-2 Recall: 0.4182988107122229\n",
            "ROUGE-2 F1: 0.4290687641277154\n",
            "ROUGE-L Precision: 0.4465320325821139\n",
            "ROUGE-L Recall: 0.37266557098613123\n",
            "ROUGE-L F1: 0.3839227109649046\n",
            "\n",
            "Subfolder: tech\n",
            "ROUGE-1 Precision: 0.7333887007624773\n",
            "ROUGE-1 Recall: 0.41769282822556103\n",
            "ROUGE-1 F1: 0.5129379743412917\n",
            "ROUGE-2 Precision: 0.5530047736752325\n",
            "ROUGE-2 Recall: 0.3190275943662546\n",
            "ROUGE-2 F1: 0.3898673117579847\n",
            "ROUGE-L Precision: 0.47782757644682355\n",
            "ROUGE-L Recall: 0.2730217472520126\n",
            "ROUGE-L F1: 0.3350880887490125\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inferences made:**\n",
        "\n",
        "*   The summarization system performs moderately across all categories, with room for improvement in capturing content nuances and maintaining coherence.\n",
        "\n",
        "*   Technical content poses the greatest challenge for the summarization system, indicating the need for specialized techniques to handle complex vocabulary and detailed information.\n",
        "\n",
        "*   Higher precision than recall suggests that the generated summaries contain relevant information but may lack completeness, highlighting the need for improved content coverage."
      ],
      "metadata": {
        "id": "Yuzh1i6zYYeu"
      }
    }
  ]
}