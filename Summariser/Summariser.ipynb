{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This Colab file has been adapted from the original Feature Matrix Generation.ipynb. It facilitates extractive text summarization for a provided input paragraph, intended for seamless integration with Streamlit.**"
      ],
      "metadata": {
        "id": "LVZdwV-nLiuD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeF-BFtOHyr-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "word_embeddings_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIaT6f_LtAb3",
        "outputId": "c699bde1-11d5-451a-ab4d-6c7ec221760e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_tfidf(text):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    return tfidf_matrix, feature_names\n"
      ],
      "metadata": {
        "id": "VnC08NgNuwe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_word_embeddings(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    word_vectors = []\n",
        "    for word in words:\n",
        "        if word in word_embeddings_model:\n",
        "            word_vectors.append(word_embeddings_model[word])\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(word_embeddings_model.vector_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "or-QagtvvfGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_sentence_embeddings(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_vectors = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        word_vectors = []\n",
        "        for word in words:\n",
        "            if word in word_embeddings_model:\n",
        "                word_vectors.append(word_embeddings_model[word])\n",
        "        if word_vectors:\n",
        "            sentence_vectors.append(np.mean(word_vectors, axis=0))\n",
        "    if sentence_vectors:\n",
        "        return np.mean(sentence_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(word_embeddings_model.vector_size)\n"
      ],
      "metadata": {
        "id": "XxnzjVjOvkgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def perform_pos_tagging(text):\n",
        "    words = word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    return pos_tags\n",
        "\n"
      ],
      "metadata": {
        "id": "UJEIbN9QvpiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_title_content_similarity(title, content):\n",
        "    if not title or not content:\n",
        "        print(\"Error: Title or content is empty.\")\n",
        "        return 0\n",
        "\n",
        "    try:\n",
        "        vectorizer = CountVectorizer().fit([title, content])\n",
        "        title_vec, content_vec = vectorizer.transform([title, content]).toarray()\n",
        "\n",
        "        similarity_score = cosine_similarity([title_vec], [content_vec])[0][0]\n",
        "        return similarity_score\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error computing similarity:\", e)\n",
        "        return 0\n"
      ],
      "metadata": {
        "id": "i5FgDwW62hSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_calculation(text, title):\n",
        "  results = {}\n",
        "\n",
        "  # Calculate Title-Content Similarity Score\n",
        "  similarity_score = compute_title_content_similarity(title, text)\n",
        "  results['similarity_score'] = similarity_score\n",
        "\n",
        "  try:\n",
        "    # Extract TF-IDF features\n",
        "    tfidf_matrix, _ = extract_tfidf(text)\n",
        "    results['tfidf_matrix'] = tfidf_matrix.toarray()\n",
        "  except Exception as tfidf_error:\n",
        "    results['tfidf_error'] = str(tfidf_error)\n",
        "\n",
        "  try:\n",
        "    # Extract Word Embeddings\n",
        "    word_embeddings_vector = extract_word_embeddings(text)\n",
        "    results['word_embeddings_vector'] = word_embeddings_vector\n",
        "  except Exception as word_embeddings_error:\n",
        "    results['word_embeddings_error'] = str(word_embeddings_error)\n",
        "\n",
        "  try:\n",
        "    # Extract Sentence Embeddings\n",
        "    sentence_embeddings_vector = extract_sentence_embeddings(text)\n",
        "    results['sentence_embeddings_vector'] = sentence_embeddings_vector\n",
        "  except Exception as sentence_embeddings_error:\n",
        "    results['sentence_embeddings_error'] = str(sentence_embeddings_error)\n",
        "\n",
        "  try:\n",
        "    # Perform POS Tagging\n",
        "    pos_tags = perform_pos_tagging(text)\n",
        "    results['pos_tags'] = pos_tags\n",
        "  except Exception as pos_tagging_error:\n",
        "    results['pos_tagging_error'] = str(pos_tagging_error)\n",
        "\n",
        "  return results\n"
      ],
      "metadata": {
        "id": "T7cj-31tvsuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from collections import Counter\n",
        "\n",
        "def sentence_position_score(total_sentences):\n",
        "    return [(1 - (i / total_sentences)) for i in range(total_sentences)]\n",
        "\n",
        "def proper_noun_score(text):\n",
        "    tagged_words = pos_tag(word_tokenize(text))\n",
        "    proper_nouns = [word for word, pos in tagged_words if pos == 'NNP']\n",
        "    return len(proper_nouns) / len(tagged_words)\n",
        "\n",
        "def compute_scores(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    total_sentences = len(sentences)\n",
        "    total_words = len(word_tokenize(text))\n",
        "    position_scores = sentence_position_score(total_sentences)\n",
        "    noun_score = proper_noun_score(text)\n",
        "    return position_scores, noun_score\n"
      ],
      "metadata": {
        "id": "_lXiaOnDGy-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_scores(text):\n",
        "  position_scores, noun_score = compute_scores(text)\n",
        "  avg_position_score = sum(position_scores) / len(position_scores)\n",
        "  avg_noun_score = noun_score\n",
        "  print(\"Average Sentence Position Score:\", avg_position_score)\n",
        "  print(\"Average Proper Noun Score:\", avg_noun_score)\n",
        "  print()\n",
        "  return avg_position_score,avg_noun_score"
      ],
      "metadata": {
        "id": "yoNxeuWzrhsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def feature_matrix_generation(title, text):\n",
        "    title_similarity_list = []\n",
        "    position_scores_list = []\n",
        "    sentence_length_list = []\n",
        "    proper_noun_score_list = []\n",
        "\n",
        "    title_similarity = compute_title_content_similarity(title, text)\n",
        "    title_similarity_list.append(title_similarity)\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    total_sentences = len(sentences)\n",
        "\n",
        "    avg_position_score, avg_noun_score = sentence_scores(text)\n",
        "\n",
        "    total_words = len(word_tokenize(text))\n",
        "    avg_sentence_length = total_words / total_sentences\n",
        "\n",
        "    position_scores = sentence_position_score(total_sentences)\n",
        "    avg_position_score = np.mean(position_scores)\n",
        "\n",
        "    noun_score = proper_noun_score(text)\n",
        "\n",
        "    position_scores_list.append(avg_position_score)\n",
        "    sentence_length_list.append(avg_sentence_length)\n",
        "    proper_noun_score_list.append(noun_score)\n",
        "\n",
        "    title_similarity_array = np.array(title_similarity_list)\n",
        "    position_scores_array = np.array(position_scores_list)\n",
        "    sentence_length_array = np.array(sentence_length_list)\n",
        "    proper_noun_score_array = np.array(proper_noun_score_list)\n",
        "\n",
        "    feature_matrix = np.column_stack((title_similarity_array, position_scores_array, sentence_length_array, proper_noun_score_array))\n",
        "\n",
        "    print(\"Feature Matrix:\")\n",
        "    print(feature_matrix)\n",
        "\n",
        "    return feature_matrix\n"
      ],
      "metadata": {
        "id": "P4LdWTi3_2_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "def sentence_position_score(total_sentences):\n",
        "    return [(1 - (i / total_sentences)) for i in range(total_sentences)]\n",
        "\n",
        "def proper_noun_score(text):\n",
        "    tagged_words = pos_tag(word_tokenize(text))\n",
        "    proper_nouns = [word for word, pos in tagged_words if pos == 'NNP']\n",
        "    return len(proper_nouns) / len(tagged_words) if len(tagged_words) > 0 else 0\n",
        "\n",
        "def compute_title_content_similarity(title, content):\n",
        "    return len(set(title.split()) & set(content.split())) / max(len(set(title.split())), 1)\n",
        "\n",
        "def compute_sentence_matrix(paragraph):\n",
        "    sentences = sent_tokenize(paragraph)\n",
        "    total_sentences = len(sentences)\n",
        "    title_similarity = 1.0\n",
        "\n",
        "    if total_sentences > 0:\n",
        "        sentence_position_scores = sentence_position_score(total_sentences)\n",
        "        sentence_proper_noun_scores = [proper_noun_score(sentence) for sentence in sentences]\n",
        "        sentence_lengths = [len(word_tokenize(sentence)) for sentence in sentences]\n",
        "\n",
        "        title_similarity_list = [title_similarity] * total_sentences\n",
        "        position_scores_array = np.array(sentence_position_scores)\n",
        "        sentence_length_array = np.array(sentence_lengths)\n",
        "        proper_noun_score_array = np.array(sentence_proper_noun_scores)\n",
        "\n",
        "        sentence_matrix = np.column_stack((title_similarity_list, position_scores_array, sentence_length_array, proper_noun_score_array))\n",
        "        return sentence_matrix\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "paragraph = \"\"\"The internet has revolutionized the way we communicate, access information, and conduct business. With billions of users worldwide, it has become an essential part of modern life. From social media platforms connecting people across the globe to e-commerce websites offering a vast array of products and services, the internet has transformed various aspects of society. However, this rapid expansion and reliance on digital technology have also brought challenges such as cybersecurity threats, privacy concerns, and digital divide issues. As we continue to embrace the digital age, it's crucial to address these challenges while harnessing the immense potential of the internet for positive change and innovation.\"\"\"\n",
        "\n",
        "sentence_matrix = compute_sentence_matrix(paragraph)\n",
        "if sentence_matrix is not None:\n",
        "    print(\"Sentence Matrix:\")\n",
        "    print(sentence_matrix)\n",
        "else:\n",
        "    print(\"No sentences found in the paragraph.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9GxGtOP8N8F",
        "outputId": "9eea9a40-d500-4013-ea32-4b8e7942214f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Matrix:\n",
            "[[ 1.   1.  16.   0. ]\n",
            " [ 1.   0.8 16.   0. ]\n",
            " [ 1.   0.6 30.   0. ]\n",
            " [ 1.   0.4 27.   0. ]\n",
            " [ 1.   0.2 30.   0. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rbm(sentence_matrix):\n",
        "\n",
        "    import numpy as np\n",
        "    from sklearn.neural_network import BernoulliRBM\n",
        "\n",
        "    n_components = 150\n",
        "    learning_rate = 0.1\n",
        "    n_iter = 70\n",
        "    enhanced_feature_matrices = []\n",
        "\n",
        "    for sample in sentence_matrix:\n",
        "        rbm1 = BernoulliRBM(n_components=n_components, random_state=0)\n",
        "        rbm1.fit(sample.reshape(1, -1))\n",
        "        s_prime = rbm1.transform(sample.reshape(1, -1))\n",
        "\n",
        "        rbm2 = BernoulliRBM(n_components=n_components,learning_rate = learning_rate, n_iter = n_iter, random_state=1)\n",
        "        rbm2.fit(s_prime)\n",
        "        s_double_prime = rbm2.transform(s_prime)\n",
        "\n",
        "        enhanced_feature_matrices.append(s_double_prime)\n",
        "\n",
        "    enhanced_feature_matrix = np.vstack(enhanced_feature_matrices)\n",
        "\n",
        "    return enhanced_feature_matrix\n"
      ],
      "metadata": {
        "id": "1ZJFh4WwCxFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def summary_generator(text, title, N=5):\n",
        "    feature_matrix = feature_matrix_generation(title, text)\n",
        "\n",
        "    if feature_matrix.ndim == 1:\n",
        "        feature_matrix = feature_matrix.reshape(1, -1)\n",
        "\n",
        "    sentence_matrix = compute_sentence_matrix(text)\n",
        "    enhanced_feature_matrix = rbm(sentence_matrix)\n",
        "\n",
        "    if enhanced_feature_matrix is None or enhanced_feature_matrix.ndim != 2:\n",
        "        print(\"Error: Enhanced feature matrix is not in the expected 2D format.\")\n",
        "        return None\n",
        "\n",
        "    feature_sums = np.sum(enhanced_feature_matrix, axis=1)\n",
        "    sorted_indices = np.argsort(feature_sums)[::-1]\n",
        "\n",
        "    selected_indices = sorted_indices[:N]\n",
        "    selected_indices = np.sort(selected_indices)\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    summary = \"\"\n",
        "    selected_count = 0\n",
        "\n",
        "    for index in selected_indices:\n",
        "        if selected_count < N:\n",
        "            if index < len(sentences):\n",
        "                summary += sentences[index] + \"\\n\"\n",
        "                selected_count += 1\n",
        "            else:\n",
        "                print(\"Index out of range:\", index)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "\n",
        "    return summary\n",
        "\n",
        "title = \"The Impact of the Internet on Modern Society\"\n",
        "text = \"\"\"The internet has revolutionized the way we communicate, access information, and conduct business. With billions of users worldwide, it has become an essential part of modern life. From social media platforms connecting people across the globe to e-commerce websites offering a vast array of products and services, the internet has transformed various aspects of society. However, this rapid expansion and reliance on digital technology have also brought challenges such as cybersecurity threats, privacy concerns, and digital divide issues. As we continue to embrace the digital age, it's crucial to address these challenges while harnessing the immense potential of the internet for positive change and innovation.\n",
        "\"\"\"\n",
        "\n",
        "generated_summary = summary_generator(text, title, N=3)\n",
        "print(\"Generated Summary:\", generated_summary)\n"
      ],
      "metadata": {
        "id": "cAcujP3nfM6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e279e736-6e98-49fe-bf86-e3e48e4288ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sentence Position Score: 0.6\n",
            "Average Proper Noun Score: 0.0\n",
            "\n",
            "Feature Matrix:\n",
            "[[ 0.5  0.6 23.8  0. ]]\n",
            "Generated Summary: From social media platforms connecting people across the globe to e-commerce websites offering a vast array of products and services, the internet has transformed various aspects of society.\n",
            "However, this rapid expansion and reliance on digital technology have also brought challenges such as cybersecurity threats, privacy concerns, and digital divide issues.\n",
            "As we continue to embrace the digital age, it's crucial to address these challenges while harnessing the immense potential of the internet for positive change and innovation.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}